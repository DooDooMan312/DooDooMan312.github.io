<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/logo.svg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/logo.svg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.23.2","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":28,"offset":18},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="我自己学习TTT做的笔记，可能比较粗糙，如有问题请批评指正。:)本文的参考文献 Sun et al., 2025 123456789@misc&#123;sun2025learninglearntesttime,  title&#x3D;&#123;Learning to (Learn at Test Time): RNNs with Expressive Hidden States&#125;,  aut">
<meta property="og:type" content="article">
<meta property="og:title" content="Test-time Training Layer">
<meta property="og:url" content="http://example.com/2025/08/07/Test-time-Training-Layer/index.html">
<meta property="og:site_name" content="DooDooMan Blog">
<meta property="og:description" content="我自己学习TTT做的笔记，可能比较粗糙，如有问题请批评指正。:)本文的参考文献 Sun et al., 2025 123456789@misc&#123;sun2025learninglearntesttime,  title&#x3D;&#123;Learning to (Learn at Test Time): RNNs with Expressive Hidden States&#125;,  aut">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-08-07T08:52:50.000Z">
<meta property="article:modified_time" content="2025-08-12T08:17:42.083Z">
<meta property="article:author" content="Liu Nianchao">
<meta property="article:tag" content="TTT layer code的个人理解">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/2025/08/07/Test-time-Training-Layer/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2025/08/07/Test-time-Training-Layer/","path":"2025/08/07/Test-time-Training-Layer/","title":"Test-time Training Layer"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Test-time Training Layer | DooDooMan Blog</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">DooDooMan Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">As a slow learner, I just have to work harder.</p>
      <img class="custom-logo-image" src="/images/logo.svg" alt="DooDooMan Blog">
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%88%91%E8%87%AA%E5%B7%B1%E5%AD%A6%E4%B9%A0TTT%E5%81%9A%E7%9A%84%E7%AC%94%E8%AE%B0%EF%BC%8C%E5%8F%AF%E8%83%BD%E6%AF%94%E8%BE%83%E7%B2%97%E7%B3%99%EF%BC%8C%E5%A6%82%E6%9C%89%E9%97%AE%E9%A2%98%E8%AF%B7%E6%89%B9%E8%AF%84%E6%8C%87%E6%AD%A3%E3%80%82"><span class="nav-number">1.</span> <span class="nav-text">我自己学习TTT做的笔记，可能比较粗糙，如有问题请批评指正。:)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TTT-Layer"><span class="nav-number">2.</span> <span class="nav-text">TTT Layer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#scan-remat-every-n-iterations-scan-f-n-carry-x"><span class="nav-number">2.1.</span> <span class="nav-text">scan_remat_every_n_iterations_scan(f, n, carry, x)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#get-multi-head-params"><span class="nav-number">2.2.</span> <span class="nav-text">get_multi_head_params</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#precompute-freqs-cis-Rotary-positional-embeddings-RoPE"><span class="nav-number">3.</span> <span class="nav-text">precompute_freqs_cis (Rotary positional embeddings, RoPE)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">3.1.</span> <span class="nav-text">参考资料</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E6%97%8B%E8%BD%AC%E7%9F%A9%E9%98%B5"><span class="nav-number">3.2.</span> <span class="nav-text">生成旋转矩阵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#apply-rotary-emb"><span class="nav-number">3.3.</span> <span class="nav-text">apply_rotary_emb</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TTTBase"><span class="nav-number">4.</span> <span class="nav-text">TTTBase</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#setup"><span class="nav-number">4.1.</span> <span class="nav-text">setup()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%84%E7%A7%8D%E5%AD%90%E7%B1%BB%E7%9A%84hooks"><span class="nav-number">4.2.</span> <span class="nav-text">各种子类的hooks</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TTT-%E7%9A%84%E2%80%9C%E8%87%AA%E9%80%82%E5%BA%94%E3%80%81%E9%80%90-head%E3%80%81%E9%80%90%E4%BD%8D%E7%BD%AE%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%8E%87%E5%9C%BA%E2%80%9D"><span class="nav-number">5.</span> <span class="nav-text">TTT 的“自适应、逐 head、逐位置的学习率场”</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Liu Nianchao"
      src="/images/bridge.jpg">
  <p class="site-author-name" itemprop="name">Liu Nianchao</p>
  <div class="site-description" itemprop="description">Blogs about Liu Nianchao</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/DooDooMan312" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;DooDooMan312" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:23121800@bjtu.edu.cn" title="E-Mail → mailto:23121800@bjtu.edu.cn" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/07/Test-time-Training-Layer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/bridge.jpg">
      <meta itemprop="name" content="Liu Nianchao">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DooDooMan Blog">
      <meta itemprop="description" content="Blogs about Liu Nianchao">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Test-time Training Layer | DooDooMan Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Test-time Training Layer
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-07 16:52:50" itemprop="dateCreated datePublished" datetime="2025-08-07T16:52:50+08:00">2025-08-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-12 16:17:42" itemprop="dateModified" datetime="2025-08-12T16:17:42+08:00">2025-08-12</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="我自己学习TTT做的笔记，可能比较粗糙，如有问题请批评指正。"><a href="#我自己学习TTT做的笔记，可能比较粗糙，如有问题请批评指正。" class="headerlink" title="我自己学习TTT做的笔记，可能比较粗糙，如有问题请批评指正。:)"></a>我自己学习TTT做的笔记，可能比较粗糙，如有问题请批评指正。:)</h1><p>本文的参考文献 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.04620">Sun et al., 2025</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">@misc&#123;sun2025learninglearntesttime,</span><br><span class="line">  title=&#123;Learning to (Learn at Test Time): RNNs with Expressive Hidden States&#125;,</span><br><span class="line">  author=&#123;Yu Sun and Xinhao Li and Karan Dalal and Jiarui Xu and Arjun Vikram and Genghan Zhang and Yann Dubois and Xinlei Chen and Xiaolong Wang and Sanmi Koyejo and Tatsunori Hashimoto and Carlos Guestrin&#125;,</span><br><span class="line">  year=&#123;2025&#125;,</span><br><span class="line">  eprint=&#123;2407.04620&#125;,</span><br><span class="line">  archivePrefix=&#123;arXiv&#125;,</span><br><span class="line">  primaryClass=&#123;cs.LG&#125;,</span><br><span class="line">  url=&#123;https://arxiv.org/abs/2407.04620&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h1 id="TTT-Layer"><a href="#TTT-Layer" class="headerlink" title="TTT Layer"></a>TTT Layer</h1><h2 id="scan-remat-every-n-iterations-scan-f-n-carry-x"><a href="#scan-remat-every-n-iterations-scan-f-n-carry-x" class="headerlink" title="scan_remat_every_n_iterations_scan(f, n, carry, x)"></a>scan_remat_every_n_iterations_scan(f, n, carry, x)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">scan_remat_every_n_iterations_scan</span>(<span class="params">f, n, carry, x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Remat every n mini batches.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将第一维 (K) 重排为 (K//n, n, ...)</span></span><br><span class="line">    x_grouped = tree_map(<span class="keyword">lambda</span> x: x.reshape((-<span class="number">1</span>, n, *x.shape[<span class="number">1</span>:])), x)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 外层 scan 按组遍历，内层 scan 执行 n 步，并用 remat 做 checkpoint</span></span><br><span class="line">    carry, y_grouped = jax.lax.scan(</span><br><span class="line">        jax.remat(partial(jax.lax.scan, f), prevent_cse=<span class="literal">False</span>),</span><br><span class="line">        carry,</span><br><span class="line">        x_grouped</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 恢复输出形状到 (K, ...)</span></span><br><span class="line">    y = tree_map(<span class="keyword">lambda</span> x: x.reshape((-<span class="number">1</span>, *x.shape[<span class="number">2</span>:])), y_grouped)</span><br><span class="line">    <span class="keyword">return</span> carry, y</span><br></pre></td></tr></table></figure>

<p>x_grouped &#x3D; tree_map(… reshape((-1, n, …)))<br>   假设原先我们要沿着最前面的维度（时间步&#x2F;小批次数）做 scan，长度是 K。这里把它 重排成形状 (-1, n, …)，也就是把 K 步分成若干 组数 &#x3D; K &#x2F;&#x2F; n，每组 n   步。x 可以是 PyTree（比如 dict&#x2F;tuple&#x2F;list&#x2F;张量的组合），tree_map 会对每片叶子张量都做同样 reshape。</p>
<h2 id="get-multi-head-params"><a href="#get-multi-head-params" class="headerlink" title="get_multi_head_params"></a>get_multi_head_params</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_multi_head_params</span>(<span class="params">self, params, param_dtype, kernel_init=<span class="string">&quot;normal&quot;</span>, std=<span class="number">0.02</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将一个单头的 params 拓展为 multi-heads 的params（tree）, 并根据名称规则对不同类型的参数进行合适的初始化</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    flat_params = flax.traverse_util.flatten_dict(params, sep=<span class="string">&quot;/&quot;</span>)</span><br><span class="line">    <span class="comment"># 将params&#123;&quot;encoder&quot;:&#123;&quot;ln&quot;:&#123;&quot;scale&quot;&#125;&#125;&#125; flatten为 key &quot;encoder/ln/scale&quot;）</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> flat_params.keys():</span><br><span class="line">        new_shape = (<span class="variable language_">self</span>.num_heads, *flat_params[k].shape)</span><br><span class="line">        <span class="comment"># new_shape 为  (num_heads, ...flat_params[tuple]中的elements)，这里做了一个unpacking-*</span></span><br><span class="line">        <span class="comment"># e.g.  flat: (in_dim, out_dim), new_shape == (num_heads, in_dim, out_dim)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据名称对不同数据进行初始化</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;scale&quot;</span> <span class="keyword">in</span> k:</span><br><span class="line">            p = <span class="variable language_">self</span>.param(k, jax.nn.initializers.ones, new_shape, param_dtype)</span><br><span class="line">        <span class="keyword">elif</span> <span class="string">&quot;kernel&quot;</span> <span class="keyword">in</span> k:</span><br><span class="line">            <span class="keyword">if</span> kernel_init == <span class="string">&quot;normal&quot;</span>:</span><br><span class="line">                initializer = nn.initializers.normal(std)</span><br><span class="line">            <span class="keyword">elif</span> kernel_init == <span class="string">&quot;zeros&quot;</span>:</span><br><span class="line">                initializer = nn.initializers.zeros</span><br><span class="line">            <span class="keyword">elif</span> kernel_init == <span class="string">&quot;ones&quot;</span>:</span><br><span class="line">                initializer = nn.initializers.ones</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> NotImplementedError(<span class="string">&quot;Initializer %s Not Implemented.&quot;</span> % (kernel_init))</span><br><span class="line">            p = <span class="variable language_">self</span>.param(k, initializer, new_shape, param_dtype)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p = <span class="variable language_">self</span>.param(k, jax.nn.initializers.zeros, new_shape, param_dtype)</span><br><span class="line">        flat_params[k] = p  </span><br><span class="line">    params_init = flax.traverse_util.unflatten_dict(flat_params, sep=<span class="string">&quot;/&quot;</span>)  <span class="comment"># 这里将 params 还原回tree结构</span></span><br><span class="line">    <span class="keyword">return</span> params_init</span><br></pre></td></tr></table></figure>

<p>TTT 的常见玩法之一是多头&#x2F;多假设并行适配（你可能看到过 multi-branch TTT、TTT ensemble 或 per-augmentation heads 等）：<br>    在同一个位置（比如 LN&#x2F;Adapter&#x2F;小型 MLP 头）准备 num_heads 份参数；<br>    测试时对同一个样本做不同扰动&#x2F;不同损失度量，每个 head 独立更新（或只更新其小头参数，不动主干）；<br>    选择最小损失的 head 来推断，或者做 head 的集成平均；<br>    也可按“时间窗口&#x2F;mini-batch 分组”让不同 head 负责不同子分布，提升鲁棒性。</p>
<p>用张量第 0 维表示 head，带来的好处：<br>    并行：同一层一次算出 K 个 head 的前向&#x2F;梯度，Vmap&#x2F;Pjit 很友好；<br>    对齐：参数树结构不变，训练&#x2F;加载更简单。</p>
<h1 id="precompute-freqs-cis-Rotary-positional-embeddings-RoPE"><a href="#precompute-freqs-cis-Rotary-positional-embeddings-RoPE" class="headerlink" title="precompute_freqs_cis (Rotary positional embeddings, RoPE)"></a>precompute_freqs_cis (Rotary positional embeddings, RoPE)</h1><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>大佬们的推导与解析非常全面<br>[1] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/642884818">https://zhuanlan.zhihu.com/p/642884818</a><br>[2] <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43646592/article/details/130924280">https://blog.csdn.net/weixin_43646592/article/details/130924280</a></p>
<h2 id="生成旋转矩阵"><a href="#生成旋转矩阵" class="headerlink" title="生成旋转矩阵"></a>生成旋转矩阵</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">precompute_freqs_cis</span>(<span class="params">dim: <span class="built_in">int</span>, end: <span class="built_in">int</span>, theta: <span class="built_in">float</span> = <span class="number">10000.0</span>, dtype: jnp.dtype = jnp.float32</span>) -&gt; jnp.ndarray:</span><br><span class="line">    freqs = <span class="number">1.0</span> / (theta ** (np.arange(<span class="number">0</span>, dim, <span class="number">2</span>)[: (dim // <span class="number">2</span>)].astype(dtype) / dim))</span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成频率系数</span></span><br><span class="line"><span class="string">        1. np.arange(0, dim, 2): 生成dim/2 个偶数</span></span><br><span class="line"><span class="string">        2. 除以dim，再作为指数经过 theta计算 获得频率衰减序列</span></span><br><span class="line"><span class="string">        3. 取倒数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    t = np.arange(end)  <span class="comment"># 生成时间步 t</span></span><br><span class="line">    freqs = np.outer(t, freqs).astype(dtype)  <span class="comment"># 做一步外积（叉程） 获得 (end, dim/2)的角度矩阵</span></span><br><span class="line">    sin, cos = np.sin(freqs), np.cos(freqs)</span><br><span class="line">    freqs_cis = np.complex64(cos + <span class="number">1j</span> * sin)</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    cos + 1j * sin 是欧拉公式 e^(iθ) 的形式（cis 就是 cosine + i·sine 的缩写）</span></span><br><span class="line"><span class="string">    将(cos, sin) 映射为复数，后续使用复数乘法实现旋转位置编码</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> jnp.asarray(freqs_cis)</span><br></pre></td></tr></table></figure>

<h2 id="apply-rotary-emb"><a href="#apply-rotary-emb" class="headerlink" title="apply_rotary_emb"></a>apply_rotary_emb</h2><p>旋转位置编码计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">apply_rotary_emb</span>(<span class="params"></span></span><br><span class="line"><span class="params">    xq: jnp.ndarray, xk: jnp.ndarray, freqs_cis: jnp.ndarray, dtype: jnp.dtype = jnp.float32</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Tuple</span>[jnp.ndarray, jnp.ndarray]:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># xq.shape == [batch_size, seq_len, dim]</span></span><br><span class="line">    <span class="comment"># reshape_xq.shape == [batch_size, seq_len, dim // 2, 2]</span></span><br><span class="line">    reshape_xq = xq.astype(jnp.float32).reshape(*xq.shape[:-<span class="number">1</span>], -<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    reshape_xk = xk.astype(jnp.float32).reshape(*xk.shape[:-<span class="number">1</span>], -<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转为复数域，jax.lax.complex() 将两个实数拼接为一个复数</span></span><br><span class="line">    xq_ = jax.lax.<span class="built_in">complex</span>(reshape_xq[..., <span class="number">0</span>], reshape_xq[..., <span class="number">1</span>])</span><br><span class="line">    xk_ = jax.lax.<span class="built_in">complex</span>(reshape_xk[..., <span class="number">0</span>], reshape_xk[..., <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 应用旋转操作，然后将结果转回实数域</span></span><br><span class="line">    freqs_cis = jnp.reshape(freqs_cis, (*freqs_cis.shape[:<span class="number">2</span>], <span class="number">1</span>, *freqs_cis.shape[<span class="number">2</span>:]))</span><br><span class="line">    <span class="comment"># xq_out.shape = [batch_size, seq_len, dim]</span></span><br><span class="line">    xq_out = xq_ * freqs_cis</span><br><span class="line">    xq_out = jnp.stack((jnp.real(xq_out), jnp.imag(xq_out)), axis=-<span class="number">1</span>).reshape(*xq_out.shape[:-<span class="number">1</span>], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    xk_out = xk_ * freqs_cis</span><br><span class="line">    xk_out = jnp.stack((jnp.real(xk_out), jnp.imag(xk_out)), axis=-<span class="number">1</span>).reshape(*xk_out.shape[:-<span class="number">1</span>], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> xq_out.astype(dtype), xk_out.astype(dtype)</span><br></pre></td></tr></table></figure>

<h1 id="TTTBase"><a href="#TTTBase" class="headerlink" title="TTTBase"></a>TTTBase</h1><h2 id="setup"><a href="#setup" class="headerlink" title="setup()"></a>setup()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TTTBase</span>(nn.Module):</span><br><span class="line">    config: <span class="type">Any</span> = <span class="literal">None</span></span><br><span class="line">    dtype: jnp.dtype = jnp.float32</span><br><span class="line">    param_dtype: jnp.dtype = jnp.float32</span><br><span class="line">    precision: <span class="type">Optional</span>[<span class="type">Union</span>[jax.lax.Precision, <span class="built_in">str</span>]] = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setup</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.width = <span class="variable language_">self</span>.config.hidden_size</span><br><span class="line">        <span class="variable language_">self</span>.num_heads = <span class="variable language_">self</span>.config.num_attention_heads</span><br><span class="line">        <span class="variable language_">self</span>.head_dim = <span class="variable language_">self</span>.width // <span class="variable language_">self</span>.num_heads</span><br><span class="line">        <span class="variable language_">self</span>.mini_batch_size = <span class="variable language_">self</span>.config.mini_batch_size</span><br><span class="line">        <span class="variable language_">self</span>.n_mini_batch = <span class="variable language_">self</span>.config.max_sequence_length // <span class="variable language_">self</span>.mini_batch_size</span><br><span class="line">        <span class="variable language_">self</span>.seq_shape = (<span class="variable language_">self</span>.n_mini_batch, <span class="variable language_">self</span>.mini_batch_size)</span><br><span class="line">        <span class="variable language_">self</span>.freqs_cis = precompute_freqs_cis(</span><br><span class="line">            <span class="variable language_">self</span>.head_dim, <span class="variable language_">self</span>.mini_batch_size * <span class="number">2</span>, theta=<span class="variable language_">self</span>.config.rope_theta, dtype=<span class="variable language_">self</span>.dtype</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># hooks</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        “子类 hook”就是父类留给子类重写的方法，相当于父类给你开了几个“插槽”，子类把自己的实现插进去。</span></span><br><span class="line"><span class="string">        父类 setup 时会自动调用这些插槽里的内容，这样就能在保持公共初始化流程的同时，让子类有自己的定制逻辑。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.setup_qkvo()</span><br><span class="line">        <span class="variable language_">self</span>.setup_token_idx()</span><br><span class="line">        <span class="variable language_">self</span>.setup_ttt_lr_gate()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.ttt_norm = LayerNormTemplate(dtype=<span class="variable language_">self</span>.dtype, param_dtype=<span class="variable language_">self</span>.param_dtype)</span><br><span class="line">        ttt_norm_params = <span class="variable language_">self</span>.ttt_norm.init(jax.random.PRNGKey(<span class="number">0</span>), jnp.ones([<span class="number">1</span>, <span class="variable language_">self</span>.head_dim]))[<span class="string">&quot;params&quot;</span>]</span><br><span class="line">        <span class="variable language_">self</span>.ttt_norm_params = get_multi_head_params(</span><br><span class="line">            <span class="variable language_">self</span>, ttt_norm_params, param_dtype=<span class="variable language_">self</span>.param_dtype, kernel_init=<span class="string">&quot;layer_norm&quot;</span></span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.post_norm = nn.LayerNorm(dtype=<span class="variable language_">self</span>.dtype, param_dtype=<span class="variable language_">self</span>.param_dtype)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.ttt_params = ()</span><br></pre></td></tr></table></figure>

<h2 id="各种子类的hooks"><a href="#各种子类的hooks" class="headerlink" title="各种子类的hooks"></a>各种子类的hooks</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">setup_ttt_lr_gate</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="comment"># creat instance</span></span><br><span class="line">    <span class="variable language_">self</span>.learnable_ttt_lr = LinearLayerTemplate(</span><br><span class="line">        width=<span class="number">1</span>, use_bias=<span class="literal">True</span>, name=<span class="string">&quot;learnable_ttt_lr&quot;</span>, dtype=<span class="variable language_">self</span>.dtype, param_dtype=<span class="variable language_">self</span>.param_dtype</span><br><span class="line">    )</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        nn.Dense(features=out_dim, dtype=jnp.float16, param_dtype=jnp.bfloat16)</span></span><br><span class="line"><span class="string">        是 Flax中的Linear layer,</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 初始化权重</span></span><br><span class="line">    learnable_ttt_lr_params = <span class="variable language_">self</span>.learnable_ttt_lr.init(jax.random.PRNGKey(<span class="number">0</span>), jnp.ones([<span class="number">1</span>, <span class="variable language_">self</span>.width]))[<span class="string">&quot;params&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="variable language_">self</span>.learnable_ttt_lr_params = get_multi_head_params(</span><br><span class="line">        <span class="variable language_">self</span>,</span><br><span class="line">        learnable_ttt_lr_params,</span><br><span class="line">        param_dtype=<span class="variable language_">self</span>.param_dtype,</span><br><span class="line">        kernel_init=<span class="string">&quot;normal&quot;</span>,</span><br><span class="line">        std=<span class="variable language_">self</span>.config.initializer_range,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_split_heads</span>(<span class="params">self, hidden_states</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        hidden_states.shape == (B, N, H)</span></span><br><span class="line"><span class="string">        hidden_states.shape[:2] == (B, N) + (num_heads, head_dim)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        so 这一步就是将hidden_states reshape为 (B, N, num_heads, head_dim)</span></span><br><span class="line"><span class="string">        将 H 拆成 num_heads, head_dim，把隐藏层维度 H 切分成 num_heads × head_dim 的结构，得到一个显式的多头张量</span></span><br><span class="line"><span class="string">        方便后续用 vmap 或矩阵运算在 head 维上并行计算</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> hidden_states.reshape(hidden_states.shape[:<span class="number">2</span>] + (<span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim))</span><br></pre></td></tr></table></figure>

<h1 id="TTT-的“自适应、逐-head、逐位置的学习率场”"><a href="#TTT-的“自适应、逐-head、逐位置的学习率场”" class="headerlink" title="TTT 的“自适应、逐 head、逐位置的学习率场”"></a>TTT 的“自适应、逐 head、逐位置的学习率场”</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">setup_ttt_lr_gate</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="comment"># creat instance</span></span><br><span class="line"></span><br><span class="line">    <span class="variable language_">self</span>.learnable_ttt_lr = LinearLayerTemplate(</span><br><span class="line">        width=<span class="number">1</span>, use_bias=<span class="literal">True</span>, name=<span class="string">&quot;learnable_ttt_lr&quot;</span>, dtype=<span class="variable language_">self</span>.dtype, param_dtype=<span class="variable language_">self</span>.param_dtype</span><br><span class="line">    )</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        nn.Dense(features=out_dim, dtype=jnp.float16, param_dtype=jnp.bfloat16)</span></span><br><span class="line"><span class="string">        是 Flax中的Linear layer,</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 初始化权重</span></span><br><span class="line">    learnable_ttt_lr_params = <span class="variable language_">self</span>.learnable_ttt_lr.init(jax.random.PRNGKey(<span class="number">0</span>), jnp.ones([<span class="number">1</span>, <span class="variable language_">self</span>.width]))[<span class="string">&quot;params&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="variable language_">self</span>.learnable_ttt_lr_params = get_multi_head_params(</span><br><span class="line">        <span class="variable language_">self</span>,</span><br><span class="line">        learnable_ttt_lr_params,</span><br><span class="line">        param_dtype=<span class="variable language_">self</span>.param_dtype,</span><br><span class="line">        kernel_init=<span class="string">&quot;normal&quot;</span>,</span><br><span class="line">        std=<span class="variable language_">self</span>.config.initializer_range,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_split_heads</span>(<span class="params">self, hidden_states</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        hidden_states.shape == (B, N, H)</span></span><br><span class="line"><span class="string">        hidden_states.shape[:2] == (B, N) + (num_heads, head_dim)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        so 这一步就是将hidden_states reshape为 (B, N, num_heads, head_dim)</span></span><br><span class="line"><span class="string">        将 H 拆成 num_heads, head_dim，把隐藏层维度 H 切分成 num_heads × head_dim 的结构，得到一个显式的多头张量</span></span><br><span class="line"><span class="string">        方便后续用 vmap 或矩阵运算在 head 维上并行计算</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> hidden_states.reshape(hidden_states.shape[:<span class="number">2</span>] + (<span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_split_mini_batches</span>(<span class="params">self, hidden_states</span>):</span><br><span class="line">    B, N, num_head, head_dim = hidden_states.shape</span><br><span class="line">    hidden_states = hidden_states.reshape(B, *<span class="variable language_">self</span>.seq_shape, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim).transpose(</span><br><span class="line">        <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span></span><br><span class="line">    )</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        hidden_states.reshape() == (B, self.n_mini_batch, self.mini_batch_size, num_heads, head_dim)</span></span><br><span class="line"><span class="string">        transpose() == (B, num_heads, n_mini_batch, mini_batch_size, head_dim)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> hidden_states</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_qkv_projections</span>(<span class="params">self, batch</span>):</span><br><span class="line">    XQ, XK, XV = <span class="variable language_">self</span>.wq(batch), <span class="variable language_">self</span>.wk(batch), <span class="variable language_">self</span>.wv(batch) <span class="comment"># Create instance</span></span><br><span class="line">    <span class="keyword">return</span> XQ, XK, XV</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_eta</span>(<span class="params">self, X</span>):</span><br><span class="line">    learnable_ttt_lr = vmap(</span><br><span class="line">        <span class="keyword">lambda</span> x, p: <span class="variable language_">self</span>.learnable_ttt_lr.apply(&#123;<span class="string">&quot;params&quot;</span>: p&#125;, x), axis_name=<span class="string">&quot;head&quot;</span>, in_axes=[<span class="literal">None</span>, <span class="number">0</span>], out_axes=<span class="number">1</span></span><br><span class="line">    )(X, <span class="variable language_">self</span>.learnable_ttt_lr_params)</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        learnable_ttt_lr 就是之前create的线性层，用来产生 lr gate</span></span><br><span class="line"><span class="string">        in_axes=[None, 0]: 对所用head 共享/广播 —— 同一个输入X，被不同head 线性层各处理一次</span></span><br><span class="line"><span class="string">        并将映射的head轴作为输出的[1]维度</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    learnable_ttt_lr = nn.sigmoid(learnable_ttt_lr)</span><br><span class="line">    learnable_ttt_lr = learnable_ttt_lr.transpose(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="comment"># 最后两位颠倒一下</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 把一个可学习的 token 权重和一个固定的 token 索引权重相加，再截断到非负。</span></span><br><span class="line">    token_idx = <span class="variable language_">self</span>.learnable_token_idx + <span class="variable language_">self</span>.token_idx</span><br><span class="line">    token_idx = jnp.clip(token_idx, a_min=<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">    eta = (</span><br><span class="line">        (<span class="variable language_">self</span>.config.ttt_base_lr * token_idx).reshape(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, token_idx.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">        * learnable_ttt_lr</span><br><span class="line">        / <span class="variable language_">self</span>.head_dim</span><br><span class="line">    )</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        ttt_base_lr * token_weight(pos) * sigmoid(Linear(X)) / head_dim</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> eta</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/TTT-layer-code%E7%9A%84%E4%B8%AA%E4%BA%BA%E7%90%86%E8%A7%A3/" rel="tag"># TTT layer code的个人理解</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/08/07/My-New-Post/" rel="prev" title="My New Post">
                  <i class="fa fa-angle-left"></i> My New Post
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Liu Nianchao</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
